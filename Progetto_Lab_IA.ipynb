{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Progetto-Lab-IA.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "mount_file_id": "1Wxt-3pofa9rg_A9z9I-2eTfpzazSwNOt",
      "authorship_tag": "ABX9TyOewvN7iZMWldBLG0F2qI0A",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/lorenzopalaia/Progetto-Lab-IA/blob/main/Progetto_Lab_IA.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Audio Style Transfer & Genre Classification\n",
        "\n",
        "Questo progetto si propone come obiettivo quello di *applicare uno stile musicale* di una fonte audio, che chiameremo **target**, ad una seconda fonte audio, che chiameremo **source**. In aggiunta intendiamo *classificare i generi musicali* di entrambe le fonti audio.\n",
        "\n",
        "## Audio Style Transfer\n",
        "\n",
        "Per raggiungere il primo obiettivo ci avvaliamo della tecnica di Neural Style Transfer come segue:\n",
        "1. Trasformazione delle fonti audio in spettrogrammi\n",
        "2. Applicazione del Neural Style Transfer sullo spettrogramma **source** basandoci sullo spettrogramma **target**\n",
        "3. Trasformazione dello spettrogramma risultante in fonte audio\n",
        "\n",
        "### Spettrogramma\n",
        "\n",
        "Uno spettrogramma è la rappresentazione grafica dell'intensità di un suono in funzione del tempo e della frequenza.\n",
        "* sull'asse delle ascisse è riportato il tempo in scala lineare\n",
        "* sull'asse delle ordinate è riportata la frequenza in scala lineare o logaritmica\n",
        "* a ciascun punto di data ascissa e data ordinata è assegnata una tonalità di grigio, o un colore, rappresentante l'intensità del suono in un dato istante di tempo e a una data frequenza\n",
        "\n",
        "![](https://upload.wikimedia.org/wikipedia/commons/7/70/SpettrogrammaParolaManoColore.jpg)\n",
        "\n",
        "## Genre Classification"
      ],
      "metadata": {
        "id": "0-WJq4Dw59Fn"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Dataset"
      ],
      "metadata": {
        "id": "prARO1c_LIBi"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Per conseguire i nostri obiettivi ci avvaliamo del dataset GTZAN scaricabile [qui](https://www.kaggle.com/datasets/andradaolteanu/gtzan-dataset-music-genre-classification). Tutte le fonti audio hanno le seguenti caratteristiche:\n",
        "* Canale `mono`\n",
        "* Campionamento a `22 KHz`\n",
        "* `16 bit per campione`\n",
        "* Durata `30 s`\n",
        "\n",
        "Il dataset è strutturato come segue:\n",
        "* in `genres_original` abbiamo 10 sottocartelle contenenti 100 fonti audio in formato `.wav` per ciascun genere\n",
        "* in `images_original` abbiamo 10 sottocartelle contenenti i rispettivi spettrogrammi\n",
        "* 2 file `.csv` contenenti le features delle fonti audio. Un file contiene per ogni brano (di 30 secondi) una media e una varianza calcolata su più features che possono essere estratte da un file audio. L'altro file ha la stessa struttura, ma prima le canzoni sono state suddivise in file audio di 3 secondi (aumentando in questo modo di 10 volte la quantità di dati che potremo fornire al nostro modello di classificazione)"
      ],
      "metadata": {
        "id": "YZ1W4jBK-W6f"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Setup"
      ],
      "metadata": {
        "id": "uVOe0vP_6x7T"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Spostiamoci nella cartella di Google Drive dove si trova il nostro dataset"
      ],
      "metadata": {
        "id": "3_2Oxbkf6I7J"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%cd /content/drive/My Drive/Colab Notebooks/GTZAN Dataset"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XDHLeEpmzjgY",
        "outputId": "4455f3ce-82ed-4fc7-e7dd-862e637f815f"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/drive/My Drive/Colab Notebooks/GTZAN Dataset\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Importiamo i pacchetti che ci servono"
      ],
      "metadata": {
        "id": "UXRllHoa6qNz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "from scipy import signal\n",
        "from scipy.io import wavfile\n",
        "import numpy as np\n",
        "from IPython.display import display, Audio\n",
        "import torch.nn as nn\n",
        "import torch\n",
        "from torch.autograd import Variable\n",
        "import librosa\n",
        "\n",
        "style_audio_name = 'genres_original/blues/blues.00000.wav'\n",
        "content_audio_name = 'genres_original/hiphop/hiphop.00000.wav'"
      ],
      "metadata": {
        "id": "JzbCSaPc6nt-"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Utilities"
      ],
      "metadata": {
        "id": "dXHjAclC61q2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Cominciamo a definire una serie di funzioni:\n",
        "* `toSpectrogram` per ottenere lo spettrogramma di una fonte audio\n",
        "* `plotSpectrogram` per stampare lo spettrogramma\n",
        "* `play` per ascoltare la fonte audio"
      ],
      "metadata": {
        "id": "tsTvgMJ96VOU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def to_spectrogram(filepath):\n",
        "  sample_rate, samples = wavfile.read(filepath)\n",
        "  frequencies, times, spectrogram = signal.spectrogram(samples, sample_rate)\n",
        "  return frequencies, times, spectrogram"
      ],
      "metadata": {
        "id": "F3PimENOoZNt"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def plot_spectrogram(frequencies, times, spectrogram):\n",
        "  plt.pcolormesh(times, frequencies, 10*np.log10(spectrogram))\n",
        "  plt.ylabel('Frequency [Hz]')\n",
        "  plt.xlabel('Time [sec]')\n",
        "  plt.show()"
      ],
      "metadata": {
        "id": "qZhMOAFosR9h"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def play(filepath):\n",
        "  audio = Audio(filepath)\n",
        "  display(audio)"
      ],
      "metadata": {
        "id": "q6aiOD8OKGs5"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Quindi testiamone il funzionamento"
      ],
      "metadata": {
        "id": "aS7Dirb2sMlZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "filepath = style_audio_name\n",
        "frequencies, times, spectrogram = to_spectrogram(filepath)\n",
        "plot_spectrogram(frequencies, times, spectrogram)\n",
        "play(filepath)"
      ],
      "metadata": {
        "id": "4GzaLuX8Huvs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Funzioni di Loss"
      ],
      "metadata": {
        "id": "XM53i4MWwugC"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Per questo approccio individuiamo due funzioni di Loss:\n",
        "1. Content Loss: minimizzarne il valore significa che la fonte audio in uscita suonerà in maniera simile alla fonte di contenuto\n",
        "2. Stlye Loss: minimizzarne il valore significa che la fonte audio in uscita suonerà in maniera simile alla fonte di stile\n",
        "\n",
        "Idealmente vorremmo che entrambe fossero minimizzate"
      ],
      "metadata": {
        "id": "NH9qN-Ac-eyG"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Content Loss\n",
        "\n",
        "La funzione di Content Loss prende una matrice di input ed un matrice di contenuto che corrisponde alla fonte audio di contenuto. Quindi ritorna la distanza pesata $w_{CL} \\cdot D^L_C(X,C)$ tra la matrice di input $X$ e la matrice di contenuto $C$. Implementiamo il tutto estendendo la classe `nn.Module` e sfruttando la funzione `nn.MSELoss`"
      ],
      "metadata": {
        "id": "TK5kU1wj75SW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class ContentLoss(nn.Module):\n",
        "\n",
        "  def __init__(self, target, weight):\n",
        "    # costruttore della classe nn.Module da cui deriviamo\n",
        "    super(ContentLoss, self).__init__()\n",
        "    # facciamo una detach, necessaria per calolare dinamicamente il gradiente\n",
        "    self.target = target.detach() * weight\n",
        "    self.weight = weight\n",
        "    self.criterion = nn.MSELoss()\n",
        "\n",
        "  def forward(self, input):\n",
        "    self.loss = self.criterion(input * self.weight, self.target)\n",
        "    self.output = input\n",
        "    return self.output\n",
        "\n",
        "  def backward(self, retain_graph=True):\n",
        "    self.loss.backward(retain_graph=retain_graph)\n",
        "    return self.loss"
      ],
      "metadata": {
        "id": "o3LPaDTKynM3"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Style Loss\n",
        "\n",
        "Ovviamente vogliamo estrarre dalla fonte di stile solamente le features più importanti. Se ad esempio è presente una parte cantata a noi non interessa. Al contrario vogliamo estrarre solamente la parte 'melodica' con le sue proprietà quali timbro e tono. Dobbiamo allora ricorrere ad una Gram Matrix. Allora prendiamo una prima parte della matrice di input ed eseguiamo una `flatten` per rimuovere una buona parte delle informazioni audio. Ripetiamo lo stesso per un'altra parte della matrice di input. Eseguiamo quindi il prodotto scalare tra le matrici 'appiattite'\n",
        "\n",
        "![](https://www.w3resource.com/w3r_images/numpy-manipulation-ndarray-flatten-function-image-1.png)\n",
        "\n",
        "Ma perché scegliamo proprio il prodotto scalare? Perché fornisce una misura di quanto due matrici siano simili o meno. Infatti se le matrici sono fortemente simili tra di loro otterremo un risultato molto grande, al contrario se sono molto differenti tra di loro otterremo un risultato molto piccolo. Per cui, se per esempio la prima matrice 'appiattita' corrisponde all'intonazione e la seconda corrisponde al volume ed otteniamo un prodotto scalare elevato vorrà dire che quando il volume è alto anche l'intonazione è alta. Il prodotto scalare però può fornirci ovviamente numeri molto grandi. Allora li normalizziamo dividendo ogni elemento per il numero totale di elementi nella matrice"
      ],
      "metadata": {
        "id": "1ilaFiYt1WZm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class GramMatrix(nn.Module):\n",
        "\n",
        "  def forward(self, input):\n",
        "    # a = batch size (= 1)\n",
        "    # b = numero di feature maps\n",
        "    # (c, d) = dimensione di una feature map (N = c * d)\n",
        "    a, b, c = input.size()\n",
        "    features = input.view(a * b, c)\n",
        "    # calcoliamo la Gram Matrix\n",
        "    G = torch.mm(features, features.t())\n",
        "    # normalizziamo i valori della Gram Matrix\n",
        "    # dividendo per il numero di elementi in ogni feature map\n",
        "    return G.div(a * b * c)\n",
        "  \n",
        "class StyleLoss(nn.Module):\n",
        "\n",
        "  def __init__(self, target, weight):\n",
        "    super(StyleLoss, self).__init__()\n",
        "    self.target = target.detach() * weight\n",
        "    self.weight = weight\n",
        "    self.gram = GramMatrix()\n",
        "    self.criterion = nn.MSELoss()\n",
        "\n",
        "  def forward(self, input):\n",
        "    self.output = input.clone()\n",
        "    self.G = self.gram(input)\n",
        "    self.G.mul_(self.weight)\n",
        "    self.loss = self.criterion(self.G, self.target)\n",
        "    return self.output\n",
        "\n",
        "  def backward(self, retain_graph=True):\n",
        "    self.loss.backward(retain_graph=retain_graph)\n",
        "    return self.loss"
      ],
      "metadata": {
        "id": "Sh5c7Uc15uDU"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Conversione da Wav a Matrice"
      ],
      "metadata": {
        "id": "vN-W60gB8iFR"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Utilizzeremo `librosa` per convertire i nostri file `.wav` in matrici da poter passare a PyTorch. Eseguiamo una Short-Time Fourier Transform ([STFT](https://en.wikipedia.org/wiki/Short-time_Fourier_transform))"
      ],
      "metadata": {
        "id": "AOjGkQ6V-jU4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "N_FTT = 2048 # window size\n",
        "\n",
        "def read_audio_spectrum(filename):\n",
        "  x, fs = librosa.load(filename)\n",
        "  S = librosa.stft(x, N_FTT)\n",
        "  p = np.angle(S)\n",
        "  S = np.log1p(np.abs(S))\n",
        "  return S, fs\n",
        "\n",
        "style_audio, style_sr = read_audio_spectrum(style_audio_name)\n",
        "content_audio, content_sr = read_audio_spectrum(content_audio_name)\n",
        "\n",
        "if (content_sr != style_sr):\n",
        "  raise 'Campionamento diverso tra le fonti audio'\n",
        "\n",
        "style_audio = style_audio.reshape([1, 1025, style_audio.shape[1]])\n",
        "content_audio = content_audio.reshape([1, 1025, content_audio.shape[1]])\n",
        "\n",
        "if torch.cuda.is_available():\n",
        "  style_float = Variable((torch.from_numpy(style_audio)).cuda())\n",
        "  content_float = Variable((torch.from_numpy(content_audio)).cuda())\n",
        "else:\n",
        "  style_float = Variable((torch.from_numpy(style_audio)))\n",
        "  content_float = Variable((torch.from_numpy(content_audio)))"
      ],
      "metadata": {
        "id": "ybvoH7CQ9pS7"
      },
      "execution_count": 18,
      "outputs": []
    }
  ]
}